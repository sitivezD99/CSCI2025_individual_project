{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80180386",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 1. CARGAR EL DATASET\u001b[39;00m\n",
      "\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Cambia 'mal_anime_2025.csv' por el nombre real de tu archivo si es distinto\u001b[39;00m\n",
      "\u001b[0;32m      5\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmal_anime_2025.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. CARGAR EL DATASET\n",
    "\n",
    "file_path = 'mal_anime_2025.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"¬°Archivo cargado con √©xito!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No encontr√© el archivo '{file_path}'. Aseg√∫rate de que est√© en la misma carpeta.\")\n",
    "    exit()\n",
    "\n",
    "# 2. INSPECCI√ìN B√ÅSICA\n",
    "print(\"\\n--- Informaci√≥n General ---\")\n",
    "print(f\"Total de Animes: {df.shape[0]}\")\n",
    "print(f\"Columnas disponibles: {list(df.columns)}\")\n",
    "\n",
    "# 3. LIMPIEZA R√ÅPIDA\n",
    "df = df.drop_duplicates(subset=['title'])\n",
    "df = df.dropna(subset=['title'])\n",
    "\n",
    "# 4. TOP 10 ANIMES POR PUNTUACI√ìN (Score)\n",
    "if 'Score' in df.columns:\n",
    "    top_10 = df.sort_values(by='Score', ascending=False).head(10)\n",
    "    print(\"\\n--- Top 10 Animes Mejor Calificados ---\")\n",
    "    print(top_10[['title', 'Score']])\n",
    "else:\n",
    "    print(\"\\nLa columna 'score' no existe. Aqu√≠ est√°n las primeras 10 filas:\")\n",
    "    print(df[['title']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930b1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning descriptions... this takes about 10-20 seconds.\n",
      "\n",
      "--- BEFORE ---\n",
      "Crime is timeless. By the year 2071, humanity has expanded across the galaxy, filling the surface of other planets with settlements like those on Eart\n",
      "\n",
      "--- AFTER ---\n",
      "crime timeless year humanity expanded galaxy filling surface planets settlements like earth new societies plagued murder drug use theft intergalactic \n",
      "\n",
      "‚úÖ Text is now 'Machine Readable'!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "#CLEANING FUNCTION\n",
    "def clean_description(text):\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    cleaned_words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "# 2. CLEANING\n",
    "df['clean_desc'] = df['description'].apply(clean_description)\n",
    "\n",
    "# 3. PREVIEW THE DIFFERENCE\n",
    "print(\"\\n--- BEFORE ---\")\n",
    "print(df['description'].iloc[0][:1500])\n",
    "print(\"\\n--- AFTER ---\")\n",
    "print(df['clean_desc'].iloc[0][:1500])\n",
    "\n",
    "print(\"\\nText is now 'Machine Readable'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bca188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Transforming text into a mathematical matrix...\n",
      "Matrix Shape: (19930, 8000)\n",
      "‚úÖ Every anime is now represented by a vector of 5,000 numbers!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. INITIALIZE THE VECTORIZER\n",
    "# max_features=5000: We only care about the top 5000 most important words\n",
    "# ngram_range=(1,2): This catches single words (\"ninja\") AND pairs (\"high school\")\n",
    "tfidf = TfidfVectorizer(max_features=8000, ngram_range=(1, 3))\n",
    "\n",
    "# 2. TRANSFORM THE TEXT INTO MATH\n",
    "print(\"Transforming text into a mathematical matrix...\")\n",
    "# We use the 'clean_desc' column we created in the previous cell\n",
    "tfidf_matrix = tfidf.fit_transform(df['clean_desc'])\n",
    "\n",
    "# 3. CHECK THE SHAPE\n",
    "print(f\"Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(\"Every anime is now represented by a vector of 8,000 numbers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6486a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample of the 8,000 Features ---\n",
      "['change fate' 'change life' 'changed' 'changes' 'changesource' 'changing'\n",
      " 'channel' 'chao' 'chaos' 'chaotic' 'chapter' 'chapters' 'char'\n",
      " 'character' 'character designs' 'characters' 'characters posted' 'charge'\n",
      " 'charged' 'charismatic']\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (the 8000 words/phrases)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Print a small sample of the words it found\n",
    "print(\"--- Sample of the 8,000 Features ---\")\n",
    "print(feature_names[1000:1020]) # Look at a slice in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d730563b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top Keywords for: Cowboy Bebop ---\n",
      "               tfidf_score\n",
      "jet               0.276636\n",
      "criminals         0.235346\n",
      "past              0.161415\n",
      "expanded          0.147836\n",
      "edward            0.146733\n",
      "iv                0.146733\n",
      "theft             0.143777\n",
      "disrupted         0.142889\n",
      "taking care       0.140460\n",
      "intergalactic     0.138318\n"
     ]
    }
   ],
   "source": [
    "# Look at the first anime in your dataframe\n",
    "anime_title = df['title'].iloc[0]\n",
    "first_vector = tfidf_matrix[0]\n",
    "\n",
    "# Convert the sparse row to a dense format and sort by score\n",
    "df_tfidf = pd.DataFrame(first_vector.T.todense(), index=feature_names, columns=[\"tfidf_score\"])\n",
    "top_keywords = df_tfidf.sort_values(by=\"tfidf_score\", ascending=False).head(10)\n",
    "\n",
    "print(f\"\\n--- Top Keywords for: {anime_title} ---\")\n",
    "print(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ff6250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top Keywords for: Cowboy Bebop ---\n",
      "               tfidf_score\n",
      "jet               0.276636\n",
      "criminals         0.235346\n",
      "past              0.161415\n",
      "expanded          0.147836\n",
      "edward            0.146733\n",
      "iv                0.146733\n",
      "theft             0.143777\n",
      "disrupted         0.142889\n",
      "taking care       0.140460\n",
      "intergalactic     0.138318\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Top Keywords for: {anime_title} ---\")\n",
    "print(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d49783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Purging website metadata from descriptions...\n",
      "\n",
      "--- Cleaned Description Preview ---\n",
      "Crime is timeless. By the year 2071, humanity has expanded across the galaxy, filling the surface of other planets with settlements like those on Earth. These new societies are plagued by murder, drug use, and theft, and intergalactic outlaws are hunted by a growing number of tough bounty hunters.Spike Spiegel and Jet Black pursue criminals throughout space to make a humble living. Beneath his goofy and aloof demeanor, Spike is haunted by the weight of his violent past. Meanwhile, Jet manages his own troubled memories while taking care of Spike and the Bebop, their ship. The duo is joined by the beautiful con artist Faye Valentine, odd child Edward Wong Hau Pepelu Tivrusky IV, and Ein, a bioengineered Welsh corgi.While developing bonds and working to catch a colorful cast of criminals, the Bebop crew's lives are disrupted by a menace from Spike's past. As a rival's maniacal plot continues to unravel, Spike must choose between life with his newfound family or revenge for his old wounds.[Written by MAL Rewrite]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def purge_mal_metadata(text):\n",
    "    # 1. Handle empty data\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # 2. Cut off the top \"website junk\"\n",
    "    # Everything before the word 'Synopsis' is usually MAL UI text\n",
    "    if \"Synopsis\" in text:\n",
    "        text = text.split(\"Synopsis\")[-1]\n",
    "    \n",
    "    # 3. Remove the voting buttons/scale\n",
    "    # These words appear in the text because of how the data was scraped\n",
    "    bad_words = [\"Masterpiece\", \"Great\", \"Very Good\", \"Good\", \"Fine\", \n",
    "                 \"Average\", \"Bad\", \"Very Bad\", \"Horrible\", \"Appalling\"]\n",
    "    for word in bad_words:\n",
    "        text = text.replace(word, \"\")\n",
    "        \n",
    "    # 4. Remove technical UI strings\n",
    "    # Removes patterns like 'Episodes: /26' or 'PV 1 English dub'\n",
    "    text = re.sub(r'Episodes:\\s?/\\d+', '', text)\n",
    "    text = re.sub(r'PV \\d+ English dub version', '', text)\n",
    "    text = text.replace(\"Add to My ListSelect\", \"\")\n",
    "    text = text.replace(\"playMore videosEdit\", \"\")\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply the function to the 'description' column\n",
    "print(\"Purging website metadata from descriptions...\")\n",
    "df['description'] = df['description'].apply(purge_mal_metadata)\n",
    "\n",
    "# Verify the result for the first anime\n",
    "print(\"\\n--- Cleaned Description Preview ---\")\n",
    "print(df['description'].iloc[0][:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a880e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Updating keywords with the new clean descriptions...\n",
      "\n",
      "--- Final Keywords for Cowboy Bebop ---\n",
      "crime timeless year humanity expanded galaxy filling surface planets settlements like earth new societies plagued murder drug use theft intergalactic outlaws hunted growing number tough bounty huntersspike spiegel jet black pursue criminals space make humble living beneath goofy aloof demeanor spike haunted weight violent past jet manages troubled memories taking care spike bebop ship duo joined beautiful artist faye valentine odd child edward wong hau pepelu tivrusky iv ein bioengineered welsh corgiwhile developing bonds working catch colorful cast criminals bebop crews lives disrupted menace spikes past rivals maniacal plot continues unravel spike choose life newfound family revenge old woundswritten mal rewrite\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def final_text_cleaning(text):\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    # 1. Lowercase everything\n",
    "    text = text.lower()\n",
    "    # 2. Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # 3. Split into words and remove Stop Words (the, is, at, etc.)\n",
    "    words = text.split()\n",
    "    cleaned_words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "    # 4. Join back into a string\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "print(\"Updating keywords with the new clean descriptions...\")\n",
    "df['clean_desc'] = df['description'].apply(final_text_cleaning)\n",
    "\n",
    "# Preview to ensure the metadata junk is gone from the keywords too\n",
    "print(\"\\n--- Final Keywords for Cowboy Bebop ---\")\n",
    "print(df['clean_desc'].iloc[0][:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6c47191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\CSCI2026 - Ignasi Bonmati\\Individual Term Project\\Anime_Universe_Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Step 1: Transforming 8,000 keywords into math matrix...\n",
      "üöÄ Step 2: Generating new Map Coordinates... (Wait 1-3 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\CSCI2026 - Ignasi Bonmati\\Individual Term Project\\Anime_Universe_Project\\venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Success! The Anime Universe has been mapped.\n",
      "                             title         x         y\n",
      "0                     Cowboy Bebop  7.123980  2.583071\n",
      "1  Cowboy Bebop: Tengoku no Tobira  9.437322  0.294572\n",
      "2                           Trigun  7.163458  2.999876\n",
      "3               Witch Hunter Robin  8.598219  1.820902\n",
      "4                   Bouken Ou Beet  9.515733  2.111956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import umap\n",
    "\n",
    "# 1. RE-VECTORIZE (Turning keywords into math)\n",
    "print(\"üî¢ Step 1: Transforming 8,000 keywords into math matrix...\")\n",
    "tfidf = TfidfVectorizer(max_features=8000, ngram_range=(1, 3))\n",
    "tfidf_matrix = tfidf.fit_transform(df['clean_desc'])\n",
    "\n",
    "# 2. RE-RUN UMAP (Squashing 8,000 dimensions into 2)\n",
    "print(\"üöÄ Step 2: Generating new Map Coordinates... (Wait 1-3 mins)\")\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15, \n",
    "    min_dist=0.1, \n",
    "    metric='cosine', \n",
    "    random_state=42\n",
    ")\n",
    "embedding = reducer.fit_transform(tfidf_matrix)\n",
    "\n",
    "# 3. SAVE TO DATAFRAME\n",
    "df['x'] = embedding[:, 0]\n",
    "df['y'] = embedding[:, 1]\n",
    "\n",
    "print(\"\\n‚úÖ Success! The Anime Universe has been mapped.\")\n",
    "print(df[['title', 'x', 'y']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f4b67df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0 Title: Cowboy Bebop\n",
      "\n",
      "--- Verified Top Keywords ---\n",
      "               tfidf_score\n",
      "jet               0.276633\n",
      "criminals         0.235344\n",
      "past              0.161414\n",
      "expanded          0.147834\n",
      "iv                0.146731\n",
      "edward            0.146731\n",
      "theft             0.143775\n",
      "disrupted         0.142888\n",
      "taking care       0.140459\n",
      "intergalactic     0.138317\n"
     ]
    }
   ],
   "source": [
    "# 1. Ensure the dataframe is clean and indexed properly\n",
    "df = df.drop_duplicates(subset=['title']).reset_index(drop=True)\n",
    "\n",
    "# 2. Re-run the Vectorizer on the CURRENT clean_desc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=8000, ngram_range=(1, 3))\n",
    "tfidf_matrix = tfidf.fit_transform(df['clean_desc'])\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# 3. Verification Test for Row 0\n",
    "anime_title = df['title'].iloc[0]\n",
    "first_vector = tfidf_matrix[0]\n",
    "\n",
    "# Convert and show\n",
    "df_tfidf = pd.DataFrame(first_vector.T.todense(), index=feature_names, columns=[\"tfidf_score\"])\n",
    "top_keywords = df_tfidf.sort_values(by=\"tfidf_score\", ascending=False).head(10)\n",
    "\n",
    "print(f\"Index 0 Title: {anime_title}\")\n",
    "print(\"\\n--- Verified Top Keywords ---\")\n",
    "print(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de579503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
